{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e30d8a2-42d7-463f-9060-d82313388897",
   "metadata": {
    "tags": []
   },
   "source": [
    "## StackOverflow Tag Predictions\n",
    "\n",
    "In this experiment I will describe the thought process, as well as a step-by-step explanation of an ML workflow, for StackOverflow tag predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f62354-088e-4d94-897f-2bb6b74a6b9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Exploration and Cleaning\n",
    "\n",
    "We need to understand the size, quality and features of our data. We will load the dataset in memory using Pandas (the dataset size allows for that) and try to understand things like size, features, and decide what to use and how.\n",
    "\n",
    "After loading the data, we will do some exploration and show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cd7369c-6b8c-4dd5-8468-df9eee8f153d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = 'archive/'\n",
    "\n",
    "# load everything in a dataframe\n",
    "questions = pd.read_csv(path + 'Questions.csv', encoding='latin')\n",
    "tags = pd.read_csv(path + 'Tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3f541eb-ee56-4db3-9fb5-9f062f79b863",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80</td>\n",
       "      <td>flex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80</td>\n",
       "      <td>actionscript-3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id             Tag\n",
       "0  80            flex\n",
       "1  80  actionscript-3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee1e6e48-73ed-43f5-ae39-a1660a6fb791",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37034"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags['Tag'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67bb7e46-7c58-4b10-98d1-47516458fd65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>ClosedDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2008-08-01T13:57:07Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>SQLStatement.execute() - multiple queries in o...</td>\n",
       "      <td>&lt;p&gt;I've written a database generation script i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2008-08-01T14:41:24Z</td>\n",
       "      <td>2012-12-26T03:45:49Z</td>\n",
       "      <td>144</td>\n",
       "      <td>Good branching and merging tutorials for Torto...</td>\n",
       "      <td>&lt;p&gt;Are there any really good tutorials explain...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  OwnerUserId          CreationDate            ClosedDate  Score   \n",
       "0  80         26.0  2008-08-01T13:57:07Z                   NaN     26  \\\n",
       "1  90         58.0  2008-08-01T14:41:24Z  2012-12-26T03:45:49Z    144   \n",
       "\n",
       "                                               Title   \n",
       "0  SQLStatement.execute() - multiple queries in o...  \\\n",
       "1  Good branching and merging tutorials for Torto...   \n",
       "\n",
       "                                                Body  \n",
       "0  <p>I've written a database generation script i...  \n",
       "1  <p>Are there any really good tutorials explain...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad5bf9cd-8c52-4eb4-82f2-8ec709536c61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score\n",
       " 0     591710\n",
       " 1     281042\n",
       " 2     125000\n",
       " 3      61182\n",
       "-1      43779\n",
       " 4      33680\n",
       " 5      20203\n",
       "-2      17833\n",
       " 6      13665\n",
       " 7       9624\n",
       "-3       8330\n",
       " 8       7266\n",
       " 9       5458\n",
       "-4       4542\n",
       " 10      4333\n",
       " 11      3434\n",
       " 12      3002\n",
       " 13      2393\n",
       "-5       2100\n",
       " 14      2035\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see how the questions are scored\n",
    "questions['Score'].value_counts().sort_values(ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aa4b4c-ba83-458f-a94b-ca112f6e1bd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "So, we seem to have some results on the dataset:\n",
    "\n",
    "1. There are multiple (around 37 thousand) unique tags, which is not feasible for a local experiment, so we will try to limit it to around 100\n",
    "2. Most of the question features seem not to be useful for the text task at hand, but we can use them to improve and limit our dataset.\n",
    "3. It seems that most questions have quite a low score, which means possibly less context or tags. We will select all questions with a score > 15 and score < 3000 which should give us a good representative dataset.\n",
    "\n",
    "<br>\n",
    "\n",
    "Regarding the text preprocessing, we need to take some things into consideration:\n",
    "1. Title and body will be handled separately, at first during the vectorization process\n",
    "2. The text preprocessing needs to be handled carefully, as the standard methods will destroy the dataset (especially the code blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d750238d-1fa7-478e-a500-e93304c9bd99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>tags</th>\n",
       "      <th>tags-len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120</td>\n",
       "      <td>ASP.NET Site Maps</td>\n",
       "      <td>&lt;p&gt;Has anyone got experience creating &lt;strong&gt;...</td>\n",
       "      <td>{sql, asp.net}</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>180</td>\n",
       "      <td>Function for creating color wheels</td>\n",
       "      <td>&lt;p&gt;This is something I've pseudo-solved many t...</td>\n",
       "      <td>{algorithm}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>260</td>\n",
       "      <td>Adding scripting functionality to .NET applica...</td>\n",
       "      <td>&lt;p&gt;I have a little game written in C#. It uses...</td>\n",
       "      <td>{c#, .net}</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id                                              Title   \n",
       "2  120                                  ASP.NET Site Maps  \\\n",
       "3  180                 Function for creating color wheels   \n",
       "4  260  Adding scripting functionality to .NET applica...   \n",
       "\n",
       "                                                Body            tags  tags-len  \n",
       "2  <p>Has anyone got experience creating <strong>...  {sql, asp.net}         2  \n",
       "3  <p>This is something I've pseudo-solved many t...     {algorithm}         1  \n",
       "4  <p>I have a little game written in C#. It uses...      {c#, .net}         2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# get the top 100 most common tags\n",
    "top_tags = tags['Tag'].value_counts().sort_values(ascending=False)[:100].index.tolist()\n",
    "\n",
    "# filter the dataframe to keep only rows with the top 100 tags\n",
    "tags = tags[tags['Tag'].isin(top_tags)]\n",
    "\n",
    "# filter by score\n",
    "questions = questions[questions['Score'] > 15]\n",
    "questions = questions[questions['Score'] < 3000]\n",
    "\n",
    "def get_tags(_id):\n",
    "    try:\n",
    "        _tags = tags[tags['Id'] == _id]\n",
    "        return set(_tags['Tag'])\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "# assign lists of tags to each question, and create a 'helper' column with the length\n",
    "questions['tags'] = questions['Id'].apply(get_tags)\n",
    "questions['tags-len'] = questions['tags'].apply(lambda x: len(x))\n",
    "\n",
    "# drop unnecessary rows/columns\n",
    "questions = questions[questions['tags-len'] > 0]\n",
    "questions.drop(columns=['OwnerUserId', 'CreationDate', 'ClosedDate', 'Score'], inplace=True)\n",
    "questions.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c45f1e-77ab-4b0e-a294-536c21f6d457",
   "metadata": {
    "tags": []
   },
   "source": [
    "So, now after this preprocessing, we have 100 tags to be used, and a list of about 20K questions. Now we can process the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a7fe375-baad-47ab-9ca0-b704c663f061",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lilykos/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/lilykos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>tags</th>\n",
       "      <th>tags-len</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120</td>\n",
       "      <td>ASP.NET Site Maps</td>\n",
       "      <td>&lt;p&gt;Has anyone got experience creating &lt;strong&gt;...</td>\n",
       "      <td>{sql, asp.net}</td>\n",
       "      <td>2</td>\n",
       "      <td>asp.net site map</td>\n",
       "      <td>anyone got experience creating sql-based asp.n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>180</td>\n",
       "      <td>Function for creating color wheels</td>\n",
       "      <td>&lt;p&gt;This is something I've pseudo-solved many t...</td>\n",
       "      <td>{algorithm}</td>\n",
       "      <td>1</td>\n",
       "      <td>function for creating color wheel</td>\n",
       "      <td>something 've pseudo-solved many time and neve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id                               Title   \n",
       "2  120                   ASP.NET Site Maps  \\\n",
       "3  180  Function for creating color wheels   \n",
       "\n",
       "                                                Body            tags   \n",
       "2  <p>Has anyone got experience creating <strong>...  {sql, asp.net}  \\\n",
       "3  <p>This is something I've pseudo-solved many t...     {algorithm}   \n",
       "\n",
       "   tags-len                        clean_title   \n",
       "2         2                   asp.net site map  \\\n",
       "3         1  function for creating color wheel   \n",
       "\n",
       "                                          clean_body  \n",
       "2  anyone got experience creating sql-based asp.n...  \n",
       "3  something 've pseudo-solved many time and neve...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# save common programming words\n",
    "allowed_words = ['if', 'else', 'then', 'while', 'for', 'and', 'as', 'from']\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    # remove URLs, mails, replace numbers, some tags\n",
    "    text = re.sub(r'(?:https?://|www\\.|mailto:)\\S+', '', text)\n",
    "    text = re.sub(r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}', '', text)\n",
    "    text = re.sub(r'<[^<]+?>', '', text)\n",
    "    text = re.sub(r'\\d+', 'NUM', text)\n",
    "    text = re.sub(r'<p>', '', text)\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    # stopwords\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words \n",
    "             if word not in stop_words or word in allowed_words]\n",
    "\n",
    "    # lemmas\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "# apply the changes to the text and show them\n",
    "questions['clean_title'] = questions['Title'].apply(preprocess)\n",
    "questions['clean_body'] = questions['Body'].apply(preprocess)\n",
    "\n",
    "questions.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab69a81-2451-4443-bfd3-2c4a4640ff50",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating the baseline model\n",
    "\n",
    "Now we are done with preprocessing the information, and we can start the ML procedure. Specifically:\n",
    "- we need to create a baseline, which will use a simple classification algorithm\n",
    "- we will use tf-idf and some way to reduce the amount of tokens (feature engineering)\n",
    "- this will be a multilabel classification problem, so we will need to find the correct algorithms\n",
    "\n",
    "The steps should be:\n",
    "- binarize the tags\n",
    "- vectorize the text input\n",
    "- train/test split\n",
    "- use a classifier and evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75d58832-8ba9-4dc1-a69a-529a541ebcbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore', category=UserWarning)\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion, make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as Tfidf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# in order to make the train/test split work, we need to update the dataframes\n",
    "_questions = questions.loc[:, ['clean_title', 'clean_body']]\n",
    "_tags = questions.loc[:, ['tags']]\n",
    "\n",
    "_X_train, _X_test, _y_train, _y_test = train_test_split(\n",
    "    _questions, _tags, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# define extractor functions\n",
    "title_extractor = FunctionTransformer(lambda x: x['clean_title'], validate=False)\n",
    "body_extractor = FunctionTransformer(lambda x: x['clean_body'], validate=False)\n",
    "\n",
    "# create a FeatureUnion pipeline that combines title / body\n",
    "vec_pipeline = make_pipeline(\n",
    "    FeatureUnion([(\n",
    "        'title_vec', make_pipeline(\n",
    "            title_extractor, Tfidf(analyzer='word', max_features=100))\n",
    "        ), (\n",
    "        'body_vec', make_pipeline(\n",
    "            body_extractor, Tfidf(analyzer='word', max_features=2000))\n",
    "    )])\n",
    ")\n",
    "\n",
    "\n",
    "# binarize the tags\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(_y_train['tags'])\n",
    "\n",
    "y_train = mlb.transform(_y_train['tags'])\n",
    "y_test = mlb.transform(_y_test['tags'])\n",
    "\n",
    "\n",
    "# fit the pipeline to the training data\n",
    "vec_pipeline.fit(_X_train)\n",
    "\n",
    "X_train = vec_pipeline.transform(_X_train)\n",
    "X_test = vec_pipeline.transform(_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5da593-3bc2-4b04-814f-9371d8d39ea7",
   "metadata": {},
   "source": [
    "We will now have train/test data, the input is vectorized, and the labels as well. We will use `Random Forest`, because it supports multilabel classifiction out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ac23eb0-9370-448b-81b2-33ddddd7e087",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Random Forest\n",
      "--- Precision: 0.907\n",
      "--- F1-score: 0.504\n",
      "--- Hamming: 0.989\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, f1_score, hamming_loss\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def metrics(y_true, y_pred, name):\n",
    "    precision = precision_score(y_true, y_pred, average='micro')\n",
    "    f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    hamming = 1 - hamming_loss(y_true, y_pred)\n",
    "\n",
    "    print('-- ' + name)\n",
    "    print('--- Precision:', round(precision, 3))\n",
    "    print('--- F1-score:', round(f1, 3))\n",
    "    print('--- Hamming:', round(hamming, 3))\n",
    "\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=500, random_state=42, n_jobs=6)\n",
    "rfc.fit(X_train, y_train)\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "metrics(y_test, y_pred, 'Random Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e398c0e-027f-4a06-b91c-e1ebf46ecb73",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Using RNN (or other DNNs) as a classifier\n",
    "\n",
    "Another solution could be to train a basic Neural network, in this case we will opt for an RNN, and use it as a classifier. The NN will handle the input data, by creating word embeddings, and then feed them to the RNN. The great addition of this method is that the input data keep their word order, so the RNN will provide better results usually, depending on the dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12d3dc09-4fb1-483f-9b8a-23c456289593",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# handle the input data\n",
    "questions['combined_rnn'] = questions['clean_title'] + questions['clean_body']\n",
    "\n",
    "# WE ONLY DO THIS DUE TO TIME/INFASTRACTURE CONSTRAINTS\n",
    "_questions = questions.sample(1000)\n",
    "\n",
    "# create the vocabulary, that will be used in an embedding, to feed to the RNN\n",
    "def create_vocab(questions):\n",
    "    vocab = set()\n",
    "    for question in questions:\n",
    "        tokens = question.split()\n",
    "        vocab.update(tokens)\n",
    "    vocab = sorted(vocab)\n",
    "    vocab.append('<pad>')\n",
    "    word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "    return word_to_index\n",
    "\n",
    "def encode_question(question, word_to_index):\n",
    "    return [word_to_index[word] for word in question.split()]\n",
    "\n",
    "\n",
    "word_to_index = create_vocab(_questions['combined_rnn'])\n",
    "encoded_questions = [encode_question(question, word_to_index) for question in _questions['combined_rnn']]\n",
    "padded_questions = pad_sequence(\n",
    "    [torch.tensor(q) for q in encoded_questions],\n",
    "    batch_first=True,\n",
    "    padding_value=word_to_index['<pad>']\n",
    ")\n",
    "\n",
    "# Encode labels using MultiLabelBinarizer and add them to the dataset\n",
    "mlb = MultiLabelBinarizer()\n",
    "encoded_labels = mlb.fit_transform(_questions['tags'])\n",
    "num_labels = len(mlb.classes_)\n",
    "\n",
    "# Create dataset \n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, questions, labels):\n",
    "        self.questions = questions\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.questions[idx], self.labels[idx]\n",
    "    \n",
    "# train/test split\n",
    "train_questions, val_questions, train_labels, val_labels = train_test_split(\n",
    "    padded_questions, encoded_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "train_dataset = TextDataset(train_questions, train_labels)\n",
    "val_dataset = TextDataset(val_questions, val_labels)\n",
    "\n",
    "# data loaders for the RNN\n",
    "batch_size = 2\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "059b911b-3e48-4208-ae8b-5970d4740ee0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0717, Hamming score: 0.9854\n",
      "Epoch 2, Loss: 0.054, Hamming score: 0.9854\n",
      "Epoch 3, Loss: 0.0749, Hamming score: 0.9854\n"
     ]
    }
   ],
   "source": [
    "# build RNN model for multi-label classification\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_labels):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.fc(x[:, -1, :])  # Use the last hidden state\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# again, due to size, these numbers are quite small\n",
    "vocab_size = len(word_to_index)\n",
    "embedding_dim = 20\n",
    "hidden_dim = 50\n",
    "num_layers = 1\n",
    "\n",
    "model = RNNModel(vocab_size, embedding_dim, hidden_dim, num_layers, num_labels)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 3\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for questions_batch, labels_batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(questions_batch)\n",
    "        loss = criterion(outputs, torch.tensor(labels_batch).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    total_hamming_loss = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for questions_batch, labels_batch in val_dataloader:\n",
    "            outputs = model(questions_batch)\n",
    "            pred_labels = (outputs > 0.5).int().numpy()\n",
    "            true_labels = labels_batch.numpy()\n",
    "            total_hamming_loss += hamming_loss(true_labels, pred_labels) * true_labels.shape[0]\n",
    "            total_samples += true_labels.shape[0]\n",
    "\n",
    "    hamming_score = 1 - (total_hamming_loss / total_samples)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {round(loss.item(), 4)}, Hamming score: {round(hamming_score, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee2072c-67d5-40e0-97ed-56125565fad6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Using LLMs\n",
    "\n",
    "We can use some kind of LLM instead, and fine-tune it using our own original dataset, and use it as a classifier. The important thing with LLMs is that they allow us to use their pre-trained power, and apply it to our needs. This is why I will use a BERT model, additionally trained on stackoverflow sentences, which then will be fine-tuned to work as a multilabel classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fea46939-991f-4545-9d11-2612fec58f93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'token_type_ids', 'labels'],\n",
       "        num_rows: 1750\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'token_type_ids', 'labels'],\n",
       "        num_rows: 750\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore', category=UserWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "from transformers import (BertTokenizerFast, BertForSequenceClassification,\n",
    "                          Trainer, TrainingArguments, DataCollatorWithPadding)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# combine title/body and train/test split\n",
    "questions['combined_text'] = questions['Title'] + ' ' + questions['Body']\n",
    "_questions = questions.loc[:, ['combined_text', 'tags']]\n",
    "\n",
    "# WE ONLY DO THIS DUE TO TIME/INFASTRACTURE CONSTRAINTS\n",
    "_questions = _questions.sample(2500)\n",
    "train_df, test_df = train_test_split(_questions, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# tokenizer init\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# create MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(_questions['tags'])\n",
    "\n",
    "# preprocess the text and create the datasets\n",
    "def preprocessing(input_text):\n",
    "    return tokenizer(input_text, padding=True, truncation=True, max_length=256)\n",
    "\n",
    "trainset = []\n",
    "testset = []\n",
    "\n",
    "for index, row in train_df.iterrows():\n",
    "    process_info = preprocessing(row['combined_text'])\n",
    "    binarized_tags = mlb.transform([row['tags']])\n",
    "    trainset.append({\n",
    "        'input_ids': process_info['input_ids'],\n",
    "        'attention_mask': process_info['attention_mask'],\n",
    "        'token_type_ids': process_info['token_type_ids'],\n",
    "        'labels': torch.tensor(binarized_tags.astype(np.float32)).squeeze()\n",
    "    })\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    process_info = preprocessing(row['combined_text'])\n",
    "    binarized_tags = mlb.transform([row['tags']])\n",
    "    testset.append({\n",
    "        'input_ids': process_info['input_ids'],\n",
    "        'attention_mask': process_info['attention_mask'],\n",
    "        'token_type_ids': process_info['token_type_ids'],\n",
    "        'labels': torch.tensor(binarized_tags.astype(np.float32)).squeeze()\n",
    "    })\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train':Dataset.from_list(trainset),\n",
    "    'eval':Dataset.from_list(testset)\n",
    "})\n",
    "dataset.set_format(\"torch\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8169a40-6130-4bb7-84ba-4ed7d113ce9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/lilykos/miniforge3/envs/core2/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='330' max='330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [330/330 46:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Hamming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.188200</td>\n",
       "      <td>0.107960</td>\n",
       "      <td>0.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.087700</td>\n",
       "      <td>0.079766</td>\n",
       "      <td>0.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.079600</td>\n",
       "      <td>0.076516</td>\n",
       "      <td>0.985000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=330, training_loss=0.1707368583390207, metrics={'train_runtime': 2796.0251, 'train_samples_per_second': 1.878, 'train_steps_per_second': 0.118, 'total_flos': 691274239488000.0, 'train_loss': 0.1707368583390207, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, f1_score, hamming_loss\n",
    "\n",
    "original_tags = mlb.classes_\n",
    "id2label = {idx:label for idx, label in enumerate(original_tags)}\n",
    "label2id = {label:idx for idx, label in enumerate(original_tags)}\n",
    "\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=100,\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    id2label=id2label, label2id=label2id\n",
    ")\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=5e-5, eps=1e-08\n",
    ")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\", logging_dir=\"./logs\",\n",
    "    num_train_epochs=3, \n",
    "    per_device_train_batch_size=16, per_device_eval_batch_size=16,\n",
    "    warmup_steps=50, eval_steps=50, logging_steps=50, \n",
    "    evaluation_strategy=\"epoch\", save_strategy=\"epoch\",\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.squeeze()\n",
    "    labels = p.label_ids.squeeze()\n",
    "    \n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(preds))\n",
    "    \n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= 0.5)] = 1\n",
    "    y_true = labels\n",
    "\n",
    "    hamming = 1 - round(hamming_loss(y_true, y_pred), 3)\n",
    "    return {'hamming': hamming}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"eval\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2957b78b-4671-4bcf-b26d-f17fbeccb6ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='47' max='47' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [47/47 01:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.07651558518409729,\n",
       " 'eval_hamming': 0.985,\n",
       " 'eval_runtime': 97.4571,\n",
       " 'eval_samples_per_second': 7.696,\n",
       " 'eval_steps_per_second': 0.482,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()\n",
    "# we can save and reuse the trained model at this checkpoint\n",
    "# trainer.save_model(\"multilabel-bert\")\n",
    "# tokenizer.save_pretrained(\"multilabel-bert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ea1210-5c26-4c20-9fd5-7f947d85db41",
   "metadata": {},
   "source": [
    "## Discussion/Comments\n",
    "After finishing the experimental pipeline, we will delve into the various stages, examining the rationale behind the decisions made and discussing potential alternatives. We will also address the limitations posed by time constraints and infrastructure, specifically the use of a single laptop, and the timeframe. In real-world scenarios, issues such as dataset size and preprocessing speed would be moot.\n",
    "\n",
    "### Data Exploration and Preprocessing\n",
    "Given the large volume of the dataset, managing it on a laptop was challenging. This is why some statistical analysis was necessary to make informed decisions on filtering, and selecting a representative subset of the data. The following steps were taken:\n",
    "\n",
    "- We kept the top 100 tags and the questions associated with them.\n",
    "- We filtered the questions based on their scores, ensuring that they were well-formed. We excluded negatively scored questions and those with extremely high scores.\n",
    "- Preprocessing must be conducted with care to preserve the integrity of the code. Many code fragments could be discarded in a standard process, either because they are not recognized as words (e.g., {}, ()) or because they are considered stopwords (e.g., \"and\", \"if\"). We aimed to maintain the context of the text without removing valuable information. Techniques such as lemmatization and tokenization were used as usual.\n",
    "\n",
    "Regarding the handling of titles and bodies, several approaches were considered:\n",
    "\n",
    "- Merging them and treating the data as a single text block for each question.\n",
    "- Vectorizing them separately, then merging the 2D arrays (title + body).\n",
    "- Investigating and applying weights to either component, based on the significance of the information they provide.\n",
    "\n",
    "\n",
    "### Multilabel Classification\n",
    "Various methods for multilabel classification exist, such as Classifier Chains. However, we chose Random Forests for their speed and out-of-the-box support for multilabel classification. The crucial aspect here was selecting appropriate performance metrics:\n",
    "\n",
    "- Accuracy is suboptimal, as it does not account for partially correct label predictions, rendering it unrepresentative, especially in a dataset with such a big amount of labels.\n",
    "- The Hamming score, which considers partially correct predictions, was used instead.\n",
    "- Precision, and F1 score were also used, with precision being especially useful.\n",
    "\n",
    "\n",
    "### Deep Neural Networks (DNNs)\n",
    "An alternative approach was to utilize a DNN, specifically a simple Recurrent Neural Network (RNN), to generate embeddings of the input text. This method, akin to using LLMs, maintains word order. Although the results may not be superior to other methods depending on the dataset, we included it for the sake of a comprehensive experimental pipeline, and we expect much better results on a more powerful.\n",
    "\n",
    "### Leveraging a Large Language Model (LLM)\n",
    "In the final stage of our experiment, we employed BERT, a widely recognized LLM known for its performance in general NLP tasks. We fine-tuned BERT on our dataset and used it as a classifier for our multilabel classification task. Other options exist of course, both different LLMs, (RoBERTa, T5, etc) but also more finetuned models (e.g. BERT fine-tuned on scientific papers). We would need to thoroughly investigate if any of those are better.\n",
    "\n",
    "### Potential Improvements and Future Directions\n",
    "While the current experimental pipeline yields promising results, there are several areas where further improvements could be made:\n",
    "\n",
    "- Experiment with other LLMs, such as GPT-4, RoBERTa, or DistilBERT, to determine if they offer better performance for this specific task.\n",
    "- Implement data augmentation techniques to expand the dataset and improve the model's generalization capabilities.\n",
    "- Explore ensemble methods, such as stacking or bagging, to combine the strengths of various models and potentially improve overall performance. This could be extremely useful due to the vast amount of tags that exist. An approach could be clustering the tags according to the questions, creating representative clusters and then fine-tun the classification procedure to different models.\n",
    "- Conduct hyperparameter optimization using techniques such as grid search, random search, k-folds, etc\n",
    "- Use domain-specific knowledge to process text more correctly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
